{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9d1724f",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46240d43",
   "metadata": {},
   "source": [
    "#### Instructions:\n",
    "- Write modular code with relevant docstrings and comments for you to be able to use\n",
    "functions you have implemented in future assignments.\n",
    "- All theory questions and observations must be written in a markdown cell of your jupyter notebook.You can alsoadd necessary images in `imgs/` and then include it in markdown. Any other submission method for theoretical question won't be entertained.\n",
    "- Start the assignment early, push your code regularly and enjoy learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31d6a65",
   "metadata": {},
   "source": [
    "### Question 1 Optimal DT from table\n",
    "**[20 points]**\\\n",
    "We will use the dataset below to learn a decision tree which predicts if people pass machine\n",
    "learning (Yes or No), based on their previous GPA (High, Medium, or Low) and whether or\n",
    "not they studied. \n",
    "\n",
    "| GPA | Studied | Passed |\n",
    "|:---:|:-------:|:------:|\n",
    "|  L  |    F    |    F   |\n",
    "|  L  |    T    |    T   |\n",
    "|  M  |    F    |    F   |\n",
    "|  M  |    T    |    T   |\n",
    "|  H  |    F    |    T   |\n",
    "|  H  |    T    |    T   |\n",
    "    \n",
    " For this problem, you can write your answers using $log_2$\n",
    ", but it may be helpful to note\n",
    "that $log_2 3 ≈ 1.6$.\n",
    "\n",
    "---\n",
    "1. What is the entropy H(Passed)?\n",
    "    <br>\n",
    "    <br>\n",
    "    $H(Passed) = \\sum -p(x) \\log(p(x))\\\\$\n",
    "    $= -p_{not pass}.\\log(p_{not pass}) -p_{pass}.\\log(p_{pass})\\\\$\n",
    "    $= -\\frac{1}{3}.\\log(\\frac{1}{3}) - \\frac{2}{3}.\\log(\\frac{2}{3})\\\\$\n",
    "    $= \\log(3) - \\frac{2}{3}\\\\$\n",
    "    $= 0.918$\n",
    "    <br>\n",
    "    <br>\n",
    "2. What is the entropy H(Passed | GPA)?\n",
    "    <br>\n",
    "    <br>\n",
    "    $H(passed \\vert GPA) = \\sum_{x \\in GPA}p(x).H(passed \\vert GPA=x)\\\\$\n",
    "    $H(passed \\vert GPA) = p(L).H(passed \\vert GPA=L) + p(M).H(passed \\vert GPA=M) + p(H).H(passed \\vert GPA=H)\\\\$\n",
    "    $H(passed \\vert GPA) =  \\frac{2}{6}.(-\\frac{1}{2}.\\log(\\frac{1}{2}) - \\frac{1}{2}.\\log(\\frac{1}{2})) + \\frac{2}{6}.(-\\frac{1}{2}.\\log(\\frac{1}{2}) -\\frac{1}{2}.\\log(\\frac{1}{2})) +\\frac{2}{6}.(-1.\\log(1) - 0.\\log(0)))\\\\$\n",
    "    $H(passed \\vert GPA) =  \\frac{1}{3}.(\\log(2)) + \\frac{1}{3}.(\\log(2)) +\\frac{1}{3}.(0)\\\\$\n",
    "    $H(passed \\vert GPA) =  \\frac{1}{3}.(1) + \\frac{1}{3}.(1) +\\frac{1}{3}.(0)\\\\$\n",
    "    $H(passed \\vert GPA) =  \\frac{2}{3}\\\\$\n",
    "    $= 0.67\\\\$\n",
    "    <br>\n",
    "    <br>\n",
    "3. What is the entropy H(Passed | Studied)?\n",
    "    <br>\n",
    "    <br>\n",
    "    $H(passed \\vert studied) = \\sum_{x \\in studied}p(x).H(passed \\vert studied=x)\\\\$\n",
    "    $H(passed \\vert studied) = p(True).H(passed \\vert studied=True) + p(False).H(passed \\vert studied=False)\\\\$\n",
    "    $H(passed \\vert studied) = \\frac{3}{6}.H(passed \\vert studied=True) + \\frac{3}{6}.H(passed \\vert studied=False)\\\\$\n",
    "    $H(passed \\vert studied) = \\frac{1}{2}.( - 1.\\log(1) - 0.\\log{0}) + \\frac{1}{2}.( - \\frac{1}{3}.\\log(\\frac{1}{3}) - \\frac{2}{3}.\\log(\\frac{2}{3}))\\\\$\n",
    "    $H(passed \\vert studied) = \\frac{1}{2}.(0) + \\frac{1}{2}.(0.918)\\\\$\n",
    "    $H(passed \\vert studied) = \\frac{1}{2}.(\\log(3) - \\frac{2}{3})\\\\$\n",
    "    $= 0.459 \\\\$\n",
    "    <br>\n",
    "    <br>\n",
    "\n",
    "4. Draw the full decision tree that would be learned for this dataset. You do not need to show any calculations.\n",
    "    <br>\n",
    "    <br>\n",
    "    ![decision  tree](./img/q1.1.png)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58cf408",
   "metadata": {},
   "source": [
    "### Question 2 DT loss functions\n",
    "**[10 points]**\n",
    "1. Explain Gini impurity and Entropy. \n",
    "    <br>\n",
    "    <br>\n",
    "    Both of them are measures used in building decision trees. A good question, is the one which causes split, which causes the least amount of variance in all the parts. Both the Entropy and Gini Impurity are a measure to evaluate the amount of imuprity, or inhomogenity, in a set of item (part). The formulas of Entropy and Gini Impurity are as follows. \n",
    "    <br>\n",
    "    $H_{entropy} = \\sum_i -p_i.\\log(p_i)\\\\$\n",
    "    $H_{gini} = 1 - \\sum_i (p_i)^2\\\\$\n",
    "    Both of them are 0, only when the split contains instances (or rather samples) from a single class.\n",
    "    <br> \n",
    "    <br>The range of Entropy - [0, 1] <br> The range of Gini Impurity - [0, 0.5]<br>\n",
    "2. What are the min and max values for both Gini impurity and Entropy\n",
    "    | Impurity | Min | Max\n",
    "    |:--:|:--:|:--:|\n",
    "    |Entropy| 0 | 1 |\n",
    "    |Gini | 0 | 0.5 |\n",
    "    \n",
    "    <br>\n",
    "3. Plot the Gini impurity and Entropy for $p\\in[0,1]$.\n",
    "    <br>\n",
    "    <br>\n",
    "    ![](./img/q2.1.png)\n",
    "5. Multiply Gini impurity by a factor of 2 and overlay it over entropy.\n",
    "    <br>\n",
    "    <br>\n",
    "    ![](./img/q2.2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0968c2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a406ac",
   "metadata": {},
   "source": [
    "### Question 3 Training a Decision Tree  \n",
    "**[40 points]**\n",
    "\n",
    "You can download the spam dataset from the link given below. This dataset contains feature vectors and the lables of Spam/Non-Spam mails. \n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\n",
    "\n",
    "**NOTE: The last column in each row represents whether the mail is spam or non spam**\\\n",
    "Although not needed, incase you want to know what the individual columns in the feature vector means, you can read it in the documentation given below.\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.DOCUMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28de757a",
   "metadata": {},
   "source": [
    "**Download the data and load it from the code given below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "163c0c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# Your code goes here #\n",
    "#######################\n",
    "data = pd.read_csv('./spambase.data', header=None)\n",
    "labels = np.array((data[len(data.columns)-1].values.tolist()))\n",
    "del data[len(data.columns)-1]\n",
    "data = np.array(data.values.tolist())\n",
    "X, y = shuffle(data, labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "117bc414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.71</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.032</td>\n",
       "      <td>2.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.320</td>\n",
       "      <td>7.0</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.548</td>\n",
       "      <td>59.0</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.745</td>\n",
       "      <td>12.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4596</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.04</td>\n",
       "      <td>3.891</td>\n",
       "      <td>70.0</td>\n",
       "      <td>323.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4597</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.780</td>\n",
       "      <td>55.0</td>\n",
       "      <td>189.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4598</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.333</td>\n",
       "      <td>10.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4599</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.616</td>\n",
       "      <td>13.0</td>\n",
       "      <td>173.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.915</td>\n",
       "      <td>29.0</td>\n",
       "      <td>339.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4601 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2    3     4     5    6     7    8     9   ...   47  \\\n",
       "0     0.00  0.00  0.00  0.0  0.00  0.00  0.0  0.00  0.0  0.00  ...  0.0   \n",
       "1     0.71  0.00  0.71  0.0  0.00  0.00  0.0  0.00  0.0  0.71  ...  0.0   \n",
       "2     0.00  0.00  0.91  0.0  0.00  0.00  0.0  0.45  0.0  0.00  ...  0.0   \n",
       "3     0.00  0.00  0.00  0.0  0.00  0.00  0.0  0.00  0.0  0.00  ...  0.0   \n",
       "4     0.00  0.00  0.54  0.0  0.00  0.00  0.0  0.00  0.0  0.00  ...  0.0   \n",
       "...    ...   ...   ...  ...   ...   ...  ...   ...  ...   ...  ...  ...   \n",
       "4596  0.00  0.00  0.00  0.0  0.00  0.00  0.0  0.00  0.0  0.00  ...  0.0   \n",
       "4597  0.00  0.23  0.00  0.0  0.23  0.47  0.0  0.47  0.0  0.95  ...  0.0   \n",
       "4598  0.00  0.00  0.00  0.0  1.49  0.00  0.0  0.00  0.0  0.00  ...  0.0   \n",
       "4599  0.00  0.23  0.00  0.0  0.00  0.23  0.0  0.46  0.0  0.00  ...  0.0   \n",
       "4600  0.10  0.00  0.41  0.0  0.10  0.10  0.1  0.52  0.1  0.00  ...  0.0   \n",
       "\n",
       "         48     49     50     51     52    53     54    55     56  \n",
       "0     0.000  0.000  0.000  0.000  0.000  0.00  1.000   1.0    3.0  \n",
       "1     0.000  0.000  0.000  0.000  0.000  0.00  1.032   2.0   32.0  \n",
       "2     0.000  0.000  0.000  0.000  0.000  0.00  1.320   7.0  103.0  \n",
       "3     0.000  0.201  0.000  0.000  0.100  0.00  4.548  59.0  141.0  \n",
       "4     0.000  0.188  0.047  0.000  0.000  0.00  1.745  12.0   89.0  \n",
       "...     ...    ...    ...    ...    ...   ...    ...   ...    ...  \n",
       "4596  0.000  0.122  0.081  0.000  0.000  0.04  3.891  70.0  323.0  \n",
       "4597  0.000  0.121  0.040  0.000  0.040  0.00  3.780  55.0  189.0  \n",
       "4598  0.000  0.229  0.000  0.000  0.000  0.00  2.333  10.0   49.0  \n",
       "4599  0.063  0.063  0.000  0.159  0.000  0.00  1.616  13.0  173.0  \n",
       "4600  0.000  0.048  0.000  0.016  0.064  0.00  1.915  29.0  339.0  \n",
       "\n",
       "[4601 rows x 57 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffee80e",
   "metadata": {},
   "source": [
    "You can try to normalize each column (feature) separately with wither one of the following ideas. **Do not normalize labels**.\n",
    "- Shift-and-scale normalization: substract the minimum, then divide by new maximum. Now all values are between 0-1\n",
    "- Zero mean, unit variance : substract the mean, divide by the appropriate value to get variance=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e67b0584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# Your code goes here #\n",
    "#######################\n",
    "def norm1(X):\n",
    "    temp = X.copy()\n",
    "    min = np.amin(temp, axis = 0)\n",
    "    temp -= min\n",
    "    max = np.max(temp, axis = 0)\n",
    "    temp /= max\n",
    "    return temp\n",
    "def norm2(X):\n",
    "    temp = X.copy()\n",
    "    temp = (temp - np.mean(temp, axis = 0))/np.std(temp, axis = 0)\n",
    "    return temp\n",
    "\n",
    "\n",
    "norm1_x = norm1(X)\n",
    "norm2_x = norm2(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef858082",
   "metadata": {},
   "source": [
    "1. Split your data into train 80% and test dataset 20% \n",
    "2. **[BONUS]** Visualize the data using PCA . You can reduce the dimension of the data if you want. Bonus marks if this increases your accuracy.\n",
    "\n",
    "*NOTE: If you are applying PCA or any other type of dimensionality reduction, do it before splitting the dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "817244db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# Your code goes here #\n",
    "#######################\n",
    "test_precentage = 20\n",
    "x_train, x_test, y_train, y_test = train_test_split(norm2_x, y, test_size=test_precentage/100, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e6bf66",
   "metadata": {},
   "source": [
    "You need to perform a K fold validation on this and report the average training error over all the k validations. \n",
    "- For this , you need to split the training data into k splits.\n",
    "- For each split, train a decision tree model and report the training , validation and test scores.\n",
    "- Report the scores in a tabular form for each validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "604495ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize K and split the data\n",
    "#Run the K fold Validation and report the scores\n",
    "\n",
    "#######################\n",
    "# Your code goes here #\n",
    "#######################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dcdf68",
   "metadata": {},
   "source": [
    "### Question 4 Random Forest Algorithm\n",
    "**[30 points]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61115eaf",
   "metadata": {},
   "source": [
    "1. What is boosting, bagging and  stacking?\n",
    "Which class does random forests belong to and why? **[5 points]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9c366d",
   "metadata": {},
   "source": [
    "2. Implement random forest algorithm using different decision trees. **[25 points]** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "412cfb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_algorithm(): # Pass necessary params as per requirements\n",
    "    pass\n",
    "    #######################\n",
    "    # Your code goes here #\n",
    "    #######################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
