{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9d1724f",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46240d43",
   "metadata": {},
   "source": [
    "#### Instructions:\n",
    "- Write modular code with relevant docstrings and comments for you to be able to use\n",
    "functions you have implemented in future assignments.\n",
    "- All theory questions and observations must be written in a markdown cell of your jupyter notebook.You can alsoadd necessary images in `imgs/` and then include it in markdown. Any other submission method for theoretical question won't be entertained.\n",
    "- Start the assignment early, push your code regularly and enjoy learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31d6a65",
   "metadata": {},
   "source": [
    "### Question 1 Optimal DT from table\n",
    "**[20 points]**\\\n",
    "We will use the dataset below to learn a decision tree which predicts if people pass machine\n",
    "learning (Yes or No), based on their previous GPA (High, Medium, or Low) and whether or\n",
    "not they studied. \n",
    "\n",
    "| GPA | Studied | Passed |\n",
    "|:---:|:-------:|:------:|\n",
    "|  L  |    F    |    F   |\n",
    "|  L  |    T    |    T   |\n",
    "|  M  |    F    |    F   |\n",
    "|  M  |    T    |    T   |\n",
    "|  H  |    F    |    T   |\n",
    "|  H  |    T    |    T   |\n",
    "    \n",
    " For this problem, you can write your answers using $log_2$\n",
    ", but it may be helpful to note\n",
    "that $log_2 3 â‰ˆ 1.6$.\n",
    "\n",
    "---\n",
    "1. What is the entropy H(Passed)?\n",
    "    <br>\n",
    "    <br>\n",
    "    $H(Passed) = \\sum -p(x) \\log(p(x))\\\\$\n",
    "    $= -p_{not pass}.\\log(p_{not pass}) -p_{pass}.\\log(p_{pass})\\\\$\n",
    "    $= -\\frac{1}{3}.\\log(\\frac{1}{3}) - \\frac{2}{3}.\\log(\\frac{2}{3})\\\\$\n",
    "    $= \\log(3) - \\frac{2}{3}\\\\$\n",
    "    $= 0.918$\n",
    "    <br>\n",
    "    <br>\n",
    "2. What is the entropy H(Passed | GPA)?\n",
    "    <br>\n",
    "    <br>\n",
    "    $H(passed \\vert GPA) = \\sum_{x \\in GPA}p(x).H(passed \\vert GPA=x)\\\\$\n",
    "    $H(passed \\vert GPA) = p(L).H(passed \\vert GPA=L) + p(M).H(passed \\vert GPA=M) + p(H).H(passed \\vert GPA=H)\\\\$\n",
    "    $H(passed \\vert GPA) =  \\frac{2}{6}.(-\\frac{1}{2}.\\log(\\frac{1}{2}) - \\frac{1}{2}.\\log(\\frac{1}{2})) + \\frac{2}{6}.(-\\frac{1}{2}.\\log(\\frac{1}{2}) -\\frac{1}{2}.\\log(\\frac{1}{2})) +\\frac{2}{6}.(-1.\\log(1) - 0.\\log(0)))\\\\$\n",
    "    $H(passed \\vert GPA) =  \\frac{1}{3}.(\\log(2)) + \\frac{1}{3}.(\\log(2)) +\\frac{1}{3}.(0)\\\\$\n",
    "    $H(passed \\vert GPA) =  \\frac{1}{3}.(1) + \\frac{1}{3}.(1) +\\frac{1}{3}.(0)\\\\$\n",
    "    $H(passed \\vert GPA) =  \\frac{2}{3}\\\\$\n",
    "    $= 0.67\\\\$\n",
    "    <br>\n",
    "    <br>\n",
    "3. What is the entropy H(Passed | Studied)?\n",
    "    <br>\n",
    "    <br>\n",
    "    $H(passed \\vert studied) = \\sum_{x \\in studied}p(x).H(passed \\vert studied=x)\\\\$\n",
    "    $H(passed \\vert studied) = p(True).H(passed \\vert studied=True) + p(False).H(passed \\vert studied=False)\\\\$\n",
    "    $H(passed \\vert studied) = \\frac{3}{6}.H(passed \\vert studied=True) + \\frac{3}{6}.H(passed \\vert studied=False)\\\\$\n",
    "    $H(passed \\vert studied) = \\frac{1}{2}.( - 1.\\log(1) - 0.\\log{0}) + \\frac{1}{2}.( - \\frac{1}{3}.\\log(\\frac{1}{3}) - \\frac{2}{3}.\\log(\\frac{2}{3}))\\\\$\n",
    "    $H(passed \\vert studied) = \\frac{1}{2}.(0) + \\frac{1}{2}.(0.918)\\\\$\n",
    "    $H(passed \\vert studied) = \\frac{1}{2}.(\\log(3) - \\frac{2}{3})\\\\$\n",
    "    $= 0.459 \\\\$\n",
    "    <br>\n",
    "    <br>\n",
    "\n",
    "4. Draw the full decision tree that would be learned for this dataset. You do not need to show any calculations.\n",
    "    <br>\n",
    "    <br>\n",
    "    ![decision  tree](./imgs/q1.1.png)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58cf408",
   "metadata": {},
   "source": [
    "### Question 2 DT loss functions\n",
    "**[10 points]**\n",
    "1. Explain Gini impurity and Entropy. \n",
    "    <br>\n",
    "    <br>\n",
    "    Both of them are measures used in building decision trees. A good question, is the one which causes split, which causes the least amount of variance in all the parts. Both the Entropy and Gini Impurity are a measure to evaluate the amount of imuprity, or inhomogenity, in a set of item (part). The formulas of Entropy and Gini Impurity are as follows. \n",
    "    <br>\n",
    "    $H_{entropy} = \\sum_i -p_i.\\log(p_i)\\\\$\n",
    "    $H_{gini} = 1 - \\sum_i (p_i)^2\\\\$\n",
    "    Both of them are 0, only when the split contains instances (or rather samples) from a single class.\n",
    "    <br> \n",
    "    <br>The range of Entropy - [0, 1] <br> The range of Gini Impurity - [0, 0.5]<br>\n",
    "2. What are the min and max values for both Gini impurity and Entropy\n",
    "    | Impurity | Min | Max\n",
    "    |:--:|:--:|:--:|\n",
    "    |Entropy| 0 | 1 |\n",
    "    |Gini | 0 | 0.5 |\n",
    "    \n",
    "    <br>\n",
    "3. Plot the Gini impurity and Entropy for $p\\in[0,1]$.\n",
    "    <br>\n",
    "    <br>\n",
    "    ![](./imgs/q2.1.png)\n",
    "5. Multiply Gini impurity by a factor of 2 and overlay it over entropy.\n",
    "    <br>\n",
    "    <br>\n",
    "    ![](./imgs/q2.2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "0968c2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a406ac",
   "metadata": {},
   "source": [
    "### Question 3 Training a Decision Tree  \n",
    "**[40 points]**\n",
    "\n",
    "You can download the spam dataset from the link given below. This dataset contains feature vectors and the lables of Spam/Non-Spam mails. \n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\n",
    "\n",
    "**NOTE: The last column in each row represents whether the mail is spam or non spam**\\\n",
    "Although not needed, incase you want to know what the individual columns in the feature vector means, you can read it in the documentation given below.\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.DOCUMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28de757a",
   "metadata": {},
   "source": [
    "**Download the data and load it from the code given below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "163c0c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# Your code goes here #\n",
    "#######################\n",
    "data = pd.read_csv('./spambase.data', header=None)\n",
    "labels = np.array((data[len(data.columns)-1].values.tolist()))\n",
    "del data[len(data.columns)-1]\n",
    "data = np.array(data.values.tolist())\n",
    "X, y = shuffle(data, labels, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffee80e",
   "metadata": {},
   "source": [
    "You can try to normalize each column (feature) separately with wither one of the following ideas. **Do not normalize labels**.\n",
    "- Shift-and-scale normalization: substract the minimum, then divide by new maximum. Now all values are between 0-1\n",
    "- Zero mean, unit variance : substract the mean, divide by the appropriate value to get variance=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "e67b0584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# Your code goes here #\n",
    "#######################\n",
    "def norm1(X):\n",
    "    temp = X.copy()\n",
    "    min = np.amin(temp, axis = 0)\n",
    "    temp -= min\n",
    "    max = np.max(temp, axis = 0)\n",
    "    temp /= max\n",
    "    return temp\n",
    "def norm2(X):\n",
    "    temp = X.copy()\n",
    "    temp = (temp - np.mean(temp, axis = 0))/np.std(temp, axis = 0)\n",
    "    return temp\n",
    "\n",
    "\n",
    "norm1_x = norm1(X)\n",
    "norm2_x = norm2(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef858082",
   "metadata": {},
   "source": [
    "1. Split your data into train 80% and test dataset 20% \n",
    "2. **[BONUS]** Visualize the data using PCA . You can reduce the dimension of the data if you want. Bonus marks if this increases your accuracy.\n",
    "\n",
    "*NOTE: If you are applying PCA or any other type of dimensionality reduction, do it before splitting the dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "817244db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# Your code goes here #\n",
    "#######################\n",
    "test_precentage = 20\n",
    "def split(X, y, test_precentage):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=test_precentage/100, random_state=0)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "x_train, x_test, y_train, y_test = split(norm1_x, y, test_precentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e6bf66",
   "metadata": {},
   "source": [
    "You need to perform a K fold validation on this and report the average training error over all the k validations. \n",
    "- For this , you need to split the training data into k splits.\n",
    "- For each split, train a decision tree model and report the training , validation and test scores.\n",
    "- Report the scores in a tabular form for each validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "604495ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Accuracy, over 15 validations: 99.92236043537565%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Training Set Accuracy</th>\n",
       "      <th>Validation Set Accuracy</th>\n",
       "      <th>Testing Set Accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99.941759</td>\n",
       "      <td>90.243902</td>\n",
       "      <td>91.748100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99.941759</td>\n",
       "      <td>90.650407</td>\n",
       "      <td>91.639522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99.912638</td>\n",
       "      <td>92.682927</td>\n",
       "      <td>90.553746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99.912638</td>\n",
       "      <td>90.650407</td>\n",
       "      <td>90.662324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99.912638</td>\n",
       "      <td>90.650407</td>\n",
       "      <td>90.119435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>99.912664</td>\n",
       "      <td>95.102041</td>\n",
       "      <td>90.770901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>99.912664</td>\n",
       "      <td>93.061224</td>\n",
       "      <td>90.770901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>99.912664</td>\n",
       "      <td>91.428571</td>\n",
       "      <td>90.662324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>99.970888</td>\n",
       "      <td>89.387755</td>\n",
       "      <td>91.530945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>99.912664</td>\n",
       "      <td>91.428571</td>\n",
       "      <td>92.073833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>99.941776</td>\n",
       "      <td>89.795918</td>\n",
       "      <td>92.182410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>99.912664</td>\n",
       "      <td>90.612245</td>\n",
       "      <td>91.096634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>99.912664</td>\n",
       "      <td>88.979592</td>\n",
       "      <td>89.902280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>99.912664</td>\n",
       "      <td>93.061224</td>\n",
       "      <td>90.445168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>99.912664</td>\n",
       "      <td>91.428571</td>\n",
       "      <td>91.096634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics     Training Set Accuracy  Validation Set Accuracy  \\\n",
       "Validation                                                   \n",
       "0                       99.941759                90.243902   \n",
       "1                       99.941759                90.650407   \n",
       "2                       99.912638                92.682927   \n",
       "3                       99.912638                90.650407   \n",
       "4                       99.912638                90.650407   \n",
       "5                       99.912664                95.102041   \n",
       "6                       99.912664                93.061224   \n",
       "7                       99.912664                91.428571   \n",
       "8                       99.970888                89.387755   \n",
       "9                       99.912664                91.428571   \n",
       "10                      99.941776                89.795918   \n",
       "11                      99.912664                90.612245   \n",
       "12                      99.912664                88.979592   \n",
       "13                      99.912664                93.061224   \n",
       "14                      99.912664                91.428571   \n",
       "\n",
       "Metrics     Testing Set Accuracy  \n",
       "Validation                        \n",
       "0                      91.748100  \n",
       "1                      91.639522  \n",
       "2                      90.553746  \n",
       "3                      90.662324  \n",
       "4                      90.119435  \n",
       "5                      90.770901  \n",
       "6                      90.770901  \n",
       "7                      90.662324  \n",
       "8                      91.530945  \n",
       "9                      92.073833  \n",
       "10                     92.182410  \n",
       "11                     91.096634  \n",
       "12                     89.902280  \n",
       "13                     90.445168  \n",
       "14                     91.096634  "
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def k_fold_validation(k, x, y):\n",
    "    '''\n",
    "    the function each of the k splits, val \n",
    "    and train sets data and labels\n",
    "    '''\n",
    "    k_folds = KFold(n_splits=k)\n",
    "    splits = []\n",
    "    for train_index, val_index in k_folds.split(x, y):\n",
    "        splits += [{\"x_train\":x[train_index], \"x_val\":x[val_index], \"y_train\":y[train_index], \"y_val\":y[val_index]}]\n",
    "    return splits\n",
    "\n",
    "def k_fold_performance(splits):\n",
    "    '''\n",
    "    This function returns the models\n",
    "    (decision trees) and their corresponding accuracies\n",
    "    when the dataset is given in the format of the return\n",
    "    object of the above function\n",
    "    '''\n",
    "    performance = dict()\n",
    "    performance['Training Set Accuracy'] = []\n",
    "    performance['Validation Set Accuracy'] = []\n",
    "    performance['Testing Set Accuracy'] = []\n",
    "    trees = []\n",
    "    for i in range(len(splits)):\n",
    "        split = splits[i]\n",
    "        train_set = split['x_train']\n",
    "        val_set = split['x_val']\n",
    "        val_y = split['y_val']\n",
    "        train_y = split['y_train']\n",
    "        clf = DecisionTreeClassifier()\n",
    "        clf.fit(train_set, train_y)\n",
    "        trees += [clf]\n",
    "        performance['Training Set Accuracy'] += [clf.score(train_set, train_y)*100]\n",
    "        performance['Validation Set Accuracy'] += [clf.score(val_set, val_y)*100]\n",
    "        performance['Testing Set Accuracy'] += [clf.score(x_test, y_test)*100]\n",
    "    return performance, trees\n",
    "\n",
    "# Initialize K and split the data\n",
    "\n",
    "k = 15\n",
    "\n",
    "# Splitting and evaluating\n",
    "\n",
    "performance, trees = k_fold_performance(k_fold_validation(k, x_train, y_train))\n",
    "print(f\"Average Training Accuracy, over {k} validations: {sum(performance['Training Set Accuracy'])/len(performance['Training Set Accuracy'])}%\")\n",
    "\n",
    "# This contains the tree models, and their respective performances\n",
    "\n",
    "df = pd.DataFrame(performance)\n",
    "df.index.names = ['Validation']\n",
    "df.columns.names = ['Metrics']\n",
    "df\n",
    "\n",
    "#Run the K fold Validation and report the scores\n",
    "\n",
    "#######################\n",
    "# Your code goes here #\n",
    "#######################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dcdf68",
   "metadata": {},
   "source": [
    "### Question 4 Random Forest Algorithm\n",
    "**[30 points]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61115eaf",
   "metadata": {},
   "source": [
    "1. What is boosting, bagging and  stacking?\n",
    "Which class does random forests belong to and why? **[5 points]**\n",
    "\n",
    "- Boosting\n",
    "  <br>\n",
    "  <br>\n",
    "  - Boosting is an ensemble technique which is used to boost the performance of weak classfiers into a strong classifier. \n",
    "  - Procedure (Adaboost)\n",
    "    - The models are sequentially learnt. The first model, is fit to the whole data. (All samples are equally likely)\n",
    "    - Then the misclassified examples' weights are increased.\n",
    "    - This gives the misclassified examples, a higher chance of occuring in the next training set. \n",
    "  <br>\n",
    "  <br>\n",
    "- Bagging\n",
    "  <br>\n",
    "  <br>\n",
    "  - Bagging is an ensemble technique, for improving unstable classification models. Usually applied to decision trees, but also can be applied to naive-bayes, KNN, etc.\n",
    "  - Procedure\n",
    "    1. Multiple Versions of the training set are created by drawing N random samples, where N is the size of the original training set. \n",
    "    2. Each of these sets are used to train different models. \n",
    "    3. The outputs of the model (for testing set) are aggregated by majority vote in the case of classification and mean in the case of regression. \n",
    "  <br>\n",
    "  <br>\n",
    "- Stacking\n",
    "  <br>\n",
    "  <br>\n",
    "  - In boosting and bagging, we use the same kind of models and exclusively those models are trained on the different versions and combined. \n",
    "  - Whereas, in stacking the individual models (reffered to as components), are different. (ex. ANNs, Decision Trees etc.)\n",
    "  - Procedure\n",
    "    - Lv - 0: **Base Learners**\n",
    "      - The original dataset is fit to each of the component. (Reffered to as Sub-Models sometimes)\n",
    "      - The outputs of each of these models are related to their respective components' training data.\n",
    "    - Lv - 1: **Stacking Model Learner**\n",
    "      - The outputs of the previous are compiled into a new dataset, and is fit to a new model. (Reffered to Aggregator Model sometimes)\n",
    "      - During the testing phase, we feed the testing data into Lv-1 and then Lv-2.\n",
    "      - The output of Lv-2 is going to give us the final prediction value for a training instance. \n",
    "  <br>\n",
    "  <br>\n",
    "- Random Forests belongs to the class of **BAGGING**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9c366d",
   "metadata": {},
   "source": [
    "2. Implement random forest algorithm using different decision trees. **[25 points]** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "412cfb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the testing Set (Random Forests): 95.33116178067318%\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>530</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  530   19\n",
       "1   24  348"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ensemble_components(components, x_train, y_train, n_prime):\n",
    "    ensemble = []\n",
    "    for _ in range(components):\n",
    "        clf = DecisionTreeClassifier(max_features= 5)\n",
    "        # Selecting features\n",
    "        rand_idx = np.random.randint(0, x_train.shape[0], n_prime)\n",
    "        train_labels = y_train[rand_idx]\n",
    "        train_set = x_train[rand_idx]\n",
    "        # bagging \n",
    "        clf.fit(train_set, train_labels)\n",
    "        ensemble += [clf]\n",
    "    return ensemble\n",
    "\n",
    "def random_forest_algorithm(number_of_trees, x_train, y_train, n_prime): # Pass necessary params as per requirements\n",
    "    '''\n",
    "    This function intends to return the indivdual trees,\n",
    "    which are going to be used for predicting the ensemble\n",
    "    '''\n",
    "    ensemble = ensemble_components(number_of_trees, x_train, y_train, n_prime)\n",
    "    return ensemble\n",
    "\n",
    "\n",
    "def test(ensemble, test_set, test_labels):\n",
    "    preds = [i.predict(test_set) for i in ensemble]\n",
    "    preds = np.array(preds)\n",
    "    preds = np.mean(preds, axis = 0)\n",
    "    for i in range(len(preds)):\n",
    "        if(preds[i] >= 0.5):\n",
    "            preds[i] = 1\n",
    "        else:\n",
    "            preds[i] = 0\n",
    "    preds = preds.astype(int)\n",
    "    score = accuracy_score(test_labels, preds)\n",
    "    return preds, score\n",
    "\n",
    "ensembles = random_forest_algorithm(100, x_train, y_train,( (len(x_train)*9)//10))\n",
    "test_preds, score = test(ensembles, x_test, y_test)\n",
    "print(f\"Accuracy on the testing Set (Random Forests): {score*100}%\")\n",
    "print(\"Confusion Matrix\")\n",
    "pd.DataFrame(confusion_matrix(y_test, test_preds))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################\n",
    "# Your code goes here #\n",
    "#######################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae19c9ff",
   "metadata": {},
   "source": [
    "### Bonus Section\n",
    "- As per the documentation, it is provided that ...\n",
    "  - The 39th Index is the Word Frequency of ```direct``` in the mail text.\n",
    "  - The 49th Index is the Word Frequency of ```cs``` in the mail text.\n",
    "  - The 51st Index is the Char Frequency of ```!``` in the mail text. \n",
    "- I hypothesise that they are redundant features, which are also seen in non-spam mails, along with the spam ones. \n",
    "- So, removing them may cause, a better classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "3a4ce0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the testing Set (Random Forests): 95.76547231270358%\n",
      "Accuracy on the testing Set (Random Forests): 95.87404994571118%\n",
      "Accuracy on the testing Set (Random Forests): 96.09120521172639%\n",
      "Accuracy on the testing Set (Random Forests): 95.87404994571118%\n",
      "Accuracy on the testing Set (Random Forests): 96.19978284473399%\n",
      "Accuracy on the testing Set (Random Forests): 95.76547231270358%\n",
      "Accuracy on the testing Set (Random Forests): 95.76547231270358%\n",
      "Accuracy on the testing Set (Random Forests): 95.87404994571118%\n",
      "Accuracy on the testing Set (Random Forests): 95.87404994571118%\n",
      "Accuracy on the testing Set (Random Forests): 95.33116178067318%\n",
      "Accuracy on the testing Set (Random Forests): 95.43973941368078%\n",
      "Accuracy on the testing Set (Random Forests): 95.43973941368078%\n",
      "Accuracy on the testing Set (Random Forests): 95.65689467969598%\n",
      "Accuracy on the testing Set (Random Forests): 96.09120521172639%\n",
      "Accuracy on the testing Set (Random Forests): 95.76547231270358%\n",
      "Accuracy on the testing Set (Random Forests): 95.43973941368078%\n",
      "Accuracy on the testing Set (Random Forests): 95.87404994571118%\n",
      "Accuracy on the testing Set (Random Forests): 95.98262757871878%\n",
      "Accuracy on the testing Set (Random Forests): 95.98262757871878%\n",
      "Accuracy on the testing Set (Random Forests): 95.76547231270358%\n",
      "Accuracy on the testing Set (Random Forests): 95.76547231270358%\n",
      "Accuracy on the testing Set (Random Forests): 95.65689467969598%\n",
      "Accuracy on the testing Set (Random Forests): 95.76547231270358%\n",
      "Accuracy on the testing Set (Random Forests): 95.98262757871878%\n",
      "Accuracy on the testing Set (Random Forests): 95.76547231270358%\n",
      "Accuracy on the testing Set (Random Forests): 95.98262757871878%\n",
      "Accuracy on the testing Set (Random Forests): 95.54831704668838%\n",
      "Accuracy on the testing Set (Random Forests): 95.87404994571118%\n",
      "Accuracy on the testing Set (Random Forests): 95.43973941368078%\n",
      "Accuracy on the testing Set (Random Forests): 95.76547231270358%\n",
      "Accuracy on the testing Set (Random Forests): 95.87404994571118%\n",
      "Accuracy on the testing Set (Random Forests): 95.54831704668838%\n",
      "Accuracy on the testing Set (Random Forests): 95.54831704668838%\n",
      "Accuracy on the testing Set (Random Forests): 95.98262757871878%\n",
      "Accuracy on the testing Set (Random Forests): 95.76547231270358%\n",
      "Accuracy on the testing Set (Random Forests): 95.54831704668838%\n",
      "Accuracy on the testing Set (Random Forests): 95.87404994571118%\n",
      "Accuracy on the testing Set (Random Forests): 95.43973941368078%\n",
      "Accuracy on the testing Set (Random Forests): 95.87404994571118%\n",
      "Accuracy on the testing Set (Random Forests): 95.54831704668838%\n",
      "Accuracy on the testing Set (Random Forests): 95.98262757871878%\n",
      "Accuracy on the testing Set (Random Forests): 95.43973941368078%\n",
      "Accuracy on the testing Set (Random Forests): 95.76547231270358%\n",
      "Accuracy on the testing Set (Random Forests): 95.76547231270358%\n",
      "Accuracy on the testing Set (Random Forests): 95.65689467969598%\n",
      "Accuracy on the testing Set (Random Forests): 96.09120521172639%\n",
      "Accuracy on the testing Set (Random Forests): 95.87404994571118%\n",
      "Accuracy on the testing Set (Random Forests): 95.11400651465797%\n",
      "Accuracy on the testing Set (Random Forests): 95.33116178067318%\n",
      "Accuracy on the testing Set (Random Forests): 95.76547231270358%\n",
      "The Mean Accuracy over 50 trails: 0.9574375678610206\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./spambase.data', header=None)\n",
    "labels = np.array((data[len(data.columns)-1].values.tolist()))\n",
    "del data[len(data.columns)-1]\n",
    "data = np.array(data.values.tolist())\n",
    "X, y = shuffle(data, labels, random_state=0)\n",
    "\n",
    "X = np.delete(X, [39, 40, 51], axis = 1)\n",
    "\n",
    "norm1_x = norm1(X)\n",
    "norm2_x = norm2(X)\n",
    "\n",
    "\n",
    "test_precentage = 20\n",
    "def split(X, y, test_precentage):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=test_precentage/100, random_state=0)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "x_train, x_test, y_train, y_test = split(norm1_x, y, test_precentage)\n",
    "\n",
    "\n",
    "temp_1 = []\n",
    "for _ in range(50):\n",
    "    ensembles_exp = random_forest_algorithm(100, x_train, y_train,( (len(x_train)*9)//10))\n",
    "    test_preds, score_exp = test(ensembles_exp, x_test, y_test)\n",
    "    print(f\"Accuracy on the testing Set (Random Forests): {score_exp*100}%\")\n",
    "    temp_1 += [score_exp]\n",
    "print(f\"The Mean Accuracy over 50 trails: {np.mean(temp_1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "2cdd91ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9619978284473398"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(temp_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45029f79",
   "metadata": {},
   "source": [
    "From the above experiment, we are able to produce, better accuracy in some cases. Consider the above trail where we got, 96% accuracy in one trail of random experiment, and the normal one produced 95.7% at the best among multiple trails.\n",
    "\n",
    "Hence we have managed to achieve a ~0.3% bump in the accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
